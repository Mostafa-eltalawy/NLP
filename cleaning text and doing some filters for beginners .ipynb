{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from wordcloud import STOPWORDS\n",
    "import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" #ourhashtag , this is our new data , and i will start training on it ! training data is must , As we need to work on it tomorrow . are you ready ? @mostafa \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ourhashtag', ',', 'this', 'is', 'our', 'new', 'data', ',', 'and', 'i', 'will', 'start', 'training', 'on', 'it', '!', 'training', 'data', 'is', 'must', ',', 'As', 'we', 'need', 'to', 'work', 'on', 'it', 'tomorrow', '.', 'are', 'you', 'ready', '?', '@mostafa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting words in text and doing some other  calculations \n",
    "word_splitting=str(text).split() #split()  it will split every word after each space...if we make it like split(\".\").. it will split words after each dot\n",
    "print(word_splitting)\n",
    "len(word_splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'to', 'will', '?', 'need', 'data', 'and', '#ourhashtag', 'you', 'start', 'on', 'must', 'work', 'are', 'tomorrow', '!', 'is', 'new', 'i', 'this', ',', 'ready', '@mostafa', '.', 'As', 'our', 'it', 'training'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting unique words \n",
    "unique_words=set(word_splitting)\n",
    "print(unique_words)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'our', 'and', 'i', 'on', 'it', 'is', 'we', 'to', 'on', 'it', 'are', 'you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting stop words\n",
    "stop_words=[]\n",
    "for w in word_splitting:\n",
    "    if w in STOPWORDS:\n",
    "        stop_words.append(w)\n",
    "print(stop_words) \n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#ourhashtag', ',', 'new', 'data', ',', 'will', 'start', 'training', '!', 'training', 'data', 'must', ',', 'As', 'need', 'work', 'tomorrow', '.', 'ready', '?', '@mostafa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deleting stop words from our words\n",
    "filtered_words=[]\n",
    "for w in word_splitting:\n",
    "    if w not in STOPWORDS:\n",
    "        filtered_words.append(w)\n",
    "print(filtered_words)\n",
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuations are   !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print('punctuations are  ',string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', ',', '!', ',', '.', '?']\n",
      "no of punctuations is =  6\n",
      "==========\n",
      "['#', ',', ',', '!', ',', '.', '?', '@']\n",
      "no of punctuations is =  8\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# punctuation_count\n",
    "\n",
    "punctuations=[]\n",
    "for w in filtered_words: #if we searched in all text before splitting it will give us also # and @\n",
    "    if w in string.punctuation:\n",
    "        punctuations.append(w)\n",
    "print(punctuations)        \n",
    "print( 'no of punctuations is = ',len(punctuations))\n",
    "print(\"==========\")\n",
    "\n",
    "punctuations1=[]\n",
    "for w in text:\n",
    "    if w in string.punctuation:\n",
    "        punctuations1.append(w)\n",
    "print(punctuations1)        \n",
    "print( 'no of punctuations is = ',len(punctuations1))\n",
    "print(\"==========\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filltered words are =  ['#ourhashtag', 'new', 'data', 'will', 'start', 'training', 'training', 'data', 'must', 'As', 'need', 'work', 'tomorrow', 'ready', '@mostafa']\n",
      "------------------------\n",
      "no. of filtered words are =  15\n"
     ]
    }
   ],
   "source": [
    "#removing punctuations from splitting test\n",
    "text_words=[]\n",
    "for w in filtered_words:\n",
    "    if w not in string.punctuation:\n",
    "        text_words.append(w)\n",
    "print('filltered words are = ',text_words)\n",
    "print(\"------------------------\")\n",
    "print('no. of filtered words are = ',len(text_words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'must', 'work', 'will', 'start', 'tomorrow', 'ready', '@mostafa', 'new', 'As', 'data', 'need', '#ourhashtag', 'training'}\n",
      "no. of unique_words are =  13\n"
     ]
    }
   ],
   "source": [
    "#getting unique words only\n",
    "\n",
    "unique_words=set(text_words)\n",
    "print(unique_words)\n",
    "print(\"no. of unique_words are = \",len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of hashtag is =  1\n",
      "==========\n",
      "no of mentioned persons is =  1\n"
     ]
    }
   ],
   "source": [
    "#counting and extracting hashtage and mentioned persons\n",
    "\n",
    "hash_tag=[]\n",
    "for char in str(text):\n",
    "    if char =='#':\n",
    "        hash_tag.append(char)\n",
    "print( 'no of hashtag is = ',len(hash_tag))\n",
    "print(\"==========\")\n",
    "\n",
    "mentioned=[]\n",
    "for char in str(text):\n",
    "    if char == '@':\n",
    "        mentioned.append(char)\n",
    "print( 'no of mentioned persons is = ',len(mentioned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words start with a are  =  ['and', 'are']\n",
      "words start with a and A are  = ['and', 'As', 'are']\n"
     ]
    }
   ],
   "source": [
    "#if you want to find words starts with any characters \n",
    "wordstartwith = re.findall(r'\\ba\\w+', text) \n",
    "print(\"words start with a are  = \",wordstartwith)\n",
    "#  if we want to search for words start with a and A \n",
    "wordstartwith1 = re.findall(r'\\b[Aa]\\w+', text) \n",
    "print(\"words start with a and A are  =\", wordstartwith1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags are :  ['#ourhashtag']\n",
      "mentioned persons are :  ['@mostafa']\n"
     ]
    }
   ],
   "source": [
    "#finding hashtages and mentioned in text\n",
    "hashtag =list(filter(lambda word: word[0]=='#', text.split()))\n",
    "print(\"hashtags are : \",hashtag)\n",
    "mentioned =list(filter(lambda word: word[0]=='@', text.split()))\n",
    "print(\"mentioned persons are : \",mentioned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
